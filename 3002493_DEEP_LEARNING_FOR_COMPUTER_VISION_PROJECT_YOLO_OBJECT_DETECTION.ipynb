{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING FOR COMPUTER VISION\n",
    "\n",
    "This project is focused on object recognition. It has **two parts**.The first one is about object detection using images, while the second one uses videos.\n",
    "\n",
    "## Business impact\n",
    "\n",
    "**Part 1. Object detection using images:** There are many companies that need to \"tag\" certain material (format may be pdf, jpg, etc.) in order to allow digitalization, data collection, and analytics. Usually this would be done by a person, tagging each object one by one. Using object decection, a company could detect objects, put them in a dataframe and build a database that is needed. I present a prototype below that requires tailoring for the specific need of each company.\n",
    "\n",
    "*Possible extension idea:* coupled with image to text script, the more powerfull digitalization tool could be created because we could pick up objects, their description and all other text.\n",
    "\n",
    "**Part 2. Object detection using videos:** The biggest impact in object detection in videos has to do with privacy, security and removal of human error factor. The company could feed all videos to the script and tag objects that appear and for how long. After that it could analyse data in collected in form of a relational database. It can be used to create automated monitoring and notification system, removing also the need to spend time watching the videos. It could create filters to get alert if some object appears. In such case, the system would have higher privacy level because no other person would constantly monitor the videos. Also, it would be a more secure system because of removal of human error factor because people could make mistakes or miss something important, while the object detection script would catch it (with high certainty) and feed it to a database. \n",
    "\n",
    "*Possible extension idea:* the idea could be extented to real time monitoring systems. Take for example entry/exit system of some object, we could train the model to catch dangerous objects such as weapons and get an alert in such case.\n",
    "\n",
    "## Description\n",
    "\n",
    "* Used YOLO (You only look once) algorithm. YOLO is a deep learning object detection algorithm and is popular because it is very fast and it detects objects in only one pass. There is are 3 frameworks to use it:darknet (written in C), darkflow (Python and uses TensorFlow), and OpenCV.\n",
    "* Used OpenCV because it has support for darknet framework.\n",
    "* Used YOLOv3-320 pre-trained model for classifying of 80 different classes. __[Download link.](https://pjreddie.com/darknet/yolo/)__\n",
    "* The model was trained on COCO (Common Objects in Context) dataset. __[Download link.](https://github.com/pjreddie/darknet/blob/master/data/coco.names)__\n",
    "\n",
    "\n",
    "\n",
    "## Examples\n",
    "\n",
    "Photo material for examples obtained from from morgueFile. __[MorgueFile](https://morguefile.com/)__ is a website database for free high resolution digital stock photography for either corporate or public use.\n",
    "\n",
    "Video material for examples obtained from Pixabay. __[Pixabay](https://pixabay.com/)__ offers images and videos, all released under __[Creative Commons Zero (CC0) License](https://creativecommons.org/publicdomain/zero/1.0/)__.\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| Pictures | Videos   |\n",
    "|------|------|\n",
    "|   __[car_crash_morgueFile.jpg](https://morguefile.com/p/1130766)__  | __[Bangkok.mp4](https://pixabay.com/videos/bangkok-thailand-city-asia-road-30949/)__|\n",
    "|   __[traffic_morgueFile.jpg](https://morguefile.com/p/1128775)__  | __[New_York_City.mp4](https://pixabay.com/videos/new-york-city-manhattan-people-cars-1044/)__|\n",
    "\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| Pictures | Videos   |\n",
    "|------|------|\n",
    "|   car_crash_morgueFile_objects_detected.jpg  | Bangkok_objects_detected.avi|\n",
    "|   traffic_morgueFile_objects_detected.jpg  | New_York_City_objects_detected.avi|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Object detection using images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import cv2 # 3.4.2 or newer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need three files to run the algorithm:\n",
    "\n",
    "* Weights file: a trained model\n",
    "* Cfg file: a configuration file\n",
    "* Image file: for which we will perform object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO deep neural network with weights and configuration that we downloaded\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "\n",
    "# Put all the classes names in a list (reading from the coco.names file)\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Get layer names\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# To create rectagles of different color for each class\n",
    "np.random.seed(92)\n",
    "colors = np.random.randint(0, 255, size=(len(classes), 3), dtype=\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading image for which we want to perform object detection\n",
    "img = cv2.imread(\"traffic_morgueFile.jpg\")\n",
    "\n",
    "# Resize the picture and get its width and height\n",
    "img = cv2.resize(img, None, fx=0.3, fy=0.3) # resize the proportions\n",
    "height, width, channels = img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blob is used to extract features from the image and to resize them. YOLO accepts three sizes:\n",
    "\n",
    "* smallest 320×320 lower accuracy, and higher speed\n",
    "* middle 416×416\n",
    "* largest 609×609 higher accuracy, and lower speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = cv2.dnn.blobFromImage(img, 1 / 255.0, (416, 416), (0, 0, 0), swapRB=True, crop=False)\n",
    "\n",
    "net.setInput(blob) # input the blob image in the network\n",
    "outs = net.forward(output_layers) # array that conains all the informations about objects detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the confidence level for a detection to be shown, and NMS threshold for noise reduction\n",
    "CONFIDENCE_LEVEL = 0.5\n",
    "NMS_THRESHOLD = 0.4\n",
    "\n",
    "class_ids = []\n",
    "confidence_levels_list = []\n",
    "boxes = []\n",
    "\n",
    "# for every layer output\n",
    "for output in outs:\n",
    "    # for every detection\n",
    "    for detection in output:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores) # to get the class_id\n",
    "        confidence = scores[class_id] # to get the confidence for a certain class_id\n",
    "        \n",
    "        if confidence > CONFIDENCE_LEVEL:\n",
    "            # Object detected\n",
    "            # Use width and heigth because we need to associate it with original size\n",
    "            center_x = int(detection[0] * width) # to get center_x coordinate\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "\n",
    "            # Rectangle coordinates; (x,y) represents bottom left corner\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            \n",
    "            # boxes: list of rectangle coordinates for detected objects\n",
    "            # class_ids: list for detected objects \n",
    "            # confidence_levels_list: list of confidence levels for detected objects\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidence_levels_list.append(float(confidence))\n",
    "            class_ids.append(class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>car</td>\n",
       "      <td>0.889183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>car</td>\n",
       "      <td>0.887948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>car</td>\n",
       "      <td>0.565972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>car</td>\n",
       "      <td>0.677397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>car</td>\n",
       "      <td>0.804338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>car</td>\n",
       "      <td>0.993208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>car</td>\n",
       "      <td>0.983978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>car</td>\n",
       "      <td>0.992442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>car</td>\n",
       "      <td>0.973989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>car</td>\n",
       "      <td>0.839636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   object  confidence\n",
       "0     car    0.889183\n",
       "1     car    0.887948\n",
       "2     car    0.565972\n",
       "3     car    0.677397\n",
       "4     car    0.804338\n",
       "..    ...         ...\n",
       "90    car    0.993208\n",
       "91    car    0.983978\n",
       "92    car    0.992442\n",
       "93    car    0.973989\n",
       "94    car    0.839636\n",
       "\n",
       "[95 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a dataframe that contains detected objects and confidence\n",
    "class_name = [str(classes[class_ids[i]]) for i in class_ids]\n",
    "df = pd.DataFrame({'object': class_name, 'confidence': confidence_levels_list})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a function called Non maximum suppression to remove the noise (many boxex for the same object)\n",
    "indexes = cv2.dnn.NMSBoxes(boxes, confidence_levels_list, CONFIDENCE_LEVEL, NMS_THRESHOLD)\n",
    "\n",
    "# font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        x, y, w, h = boxes[i]\n",
    "        text = str(classes[class_ids[i]]) + ': ' + str(round(confidence_levels_list[i],3))\n",
    "        color = [int(c) for c in colors[class_ids[i]]]\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(img, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "# Write the image\n",
    "cv2.imwrite(\"traffic_morgueFile_objects_detected.jpg\", img)\n",
    "\n",
    "# To just see the image instead\n",
    "# cv2.imshow(\"Image\", img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Object detection using videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO deep neural network with weights and configuration that we downloaded\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "\n",
    "# Put all the classes names in a list (reading from the coco.names file)\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Get layer names\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "np.random.seed(92)\n",
    "\n",
    "# To create rectagles of different color for each class\n",
    "np.random.seed(92)\n",
    "colors = np.random.randint(0, 255, size=(len(classes), 3), dtype=\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the confidence level for a detection to be shown, and NMS threshold for noise reduction\n",
    "CONFIDENCE_LEVEL = 0.5\n",
    "NMS_THRESHOLD = 0.4\n",
    "\n",
    "# Start the Video Stream with the input video file, and frame dimensions\n",
    "vs = cv2.VideoCapture(\"Bangkok.mp4\")\n",
    "writer = None\n",
    "(W, H) = (None, None)\n",
    "\n",
    "# for every frame from the video\n",
    "while True:\n",
    "    # get frame from the video\n",
    "    (grabbed, frame) = vs.read()\n",
    "\n",
    "    # if grabbed is False, this breaks the while loop because we reached the end\n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    # if the frame dimensions are empty, grab them\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), (0, 0, 0), swapRB=True, crop=False)\n",
    "    \n",
    "    net.setInput(blob) # input the blob image in the network\n",
    "    outs = net.forward(output_layers) # array that conains all the informations about objects detected\n",
    "\n",
    "    boxes = []\n",
    "    confidence_levels_list = []\n",
    "    class_ids = []\n",
    "    \n",
    "    # for every layer output\n",
    "    for output in outs:\n",
    "        # for every detection\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > CONFIDENCE_LEVEL:\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                \n",
    "                (center_x, center_y, width, height) = box.astype(\"int\")\n",
    "\n",
    "                # Rectangle coordinates; (x,y) represents bottom left corner\n",
    "                x = int(center_x - (width / 2))\n",
    "                y = int(center_y - (height / 2))\n",
    "\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidence_levels_list.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "\n",
    "    # We use a function called Non maximum suppression to remove the noise (many boxex for the same object)\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidence_levels_list, CONFIDENCE_LEVEL, NMS_THRESHOLD)\n",
    "\n",
    "    # check if at least one detection in the frame\n",
    "    if len(indexes) > 0:\n",
    "        # for every index of the detected object\n",
    "        for i in indexes.flatten():\n",
    "            # coordinates\n",
    "            x, y, w, h = boxes[i][0], boxes[i][1], boxes[i][2], boxes[i][3]\n",
    "            \n",
    "            # create a rectangle and text and put it on top of the frame\n",
    "            text = str(classes[class_ids[i]]) + ': ' + str(round(confidence_levels_list[i],3))\n",
    "            color = [int(c) for c in colors[class_ids[i]]]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    if writer is None:\n",
    "        # initialize video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(\"Bangkok_objects_detected.avi\", fourcc, 30, (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    # write the output frame to disk\n",
    "    writer.write(frame)\n",
    "\n",
    "writer.release()\n",
    "vs.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* __[YOLO website](https://pjreddie.com/darknet/yolo/)__,\n",
    "* __[Common Objects in Context](http://cocodataset.org/#overview)__,\n",
    "* __[YOLO object detection with OpenCV](https://www.pyimagesearch.com/2018/11/12/yolo-object-detection-with-opencv/)__ Author: Adrian Rosebrock,\n",
    "* __[Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3](https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088)__ Author: Jonathan Hui,\n",
    "* __[Deep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ )](https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/)__ Author: Sunita Nayak,\n",
    "* __[YOLOv3: An Incremental Improvement](https://arxiv.org/abs/1804.02767)__, Authors: Redmon, Joseph and Farhadi, Ali."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-v3",
   "language": "python",
   "name": "tf-gpu-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
